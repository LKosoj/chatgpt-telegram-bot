import logging
import os
import io
import hashlib
import json
import time
from typing import Dict, List
import faiss
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from .plugin import Plugin
import asyncio
from docling.document_converter import DocumentConverter
import tempfile
import concurrent.futures
from ..utils import handle_direct_result
import torch
import gc

class TextDocumentQAPlugin(Plugin):
    """
    –ü–ª–∞–≥–∏–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    """
    def __init__(self):
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        self.docs_dir = os.path.join(os.path.dirname(__file__), 'text_documents')
        self.index_dir = os.path.join(os.path.dirname(__file__), 'vector_indices')
        self.metadata_dir = os.path.join(os.path.dirname(__file__), 'document_metadata')
        os.makedirs(self.docs_dir, exist_ok=True)
        os.makedirs(self.index_dir, exist_ok=True)
        os.makedirs(self.metadata_dir, exist_ok=True)
                
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.document_converter = DocumentConverter()
        
        # ThreadPoolExecutor –¥–ª—è CPU-bound –æ–ø–µ—Ä–∞—Ü–∏–π
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        self.document_indices = {}
        self._load_existing_indices()
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (30 –¥–Ω–µ–π –≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
        self.max_document_age = 30 * 24 * 60 * 60
        # –í—Ä–µ–º—è –∑–∞ –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏—Ç—å –æ —É–¥–∞–ª–µ–Ω–∏–∏ (1 –¥–µ–Ω—å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
        self.warning_before_delete = 24 * 60 * 60
        
        # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∑–∞–ø—É—â–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏ –æ—á–∏—Å—Ç–∫–∏
        self.cleanup_task = None
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.processing_tasks = {}
        self.config = {'enable_quoting': False}

    def get_source_name(self) -> str:
        return "TextDocumentQA"

    def get_commands(self) -> List[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–º–∞–Ω–¥, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–ª–∞–≥–∏–Ω"""
        return [
            {
                "command": "list_documents",
                "description": "–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                "handler": self.execute,
                "handler_kwargs": {"function_name": "list_documents"},
                "plugin_name": "text_document_qa",
                "add_to_menu": True
            },
            {
                "command": "ask_question",
                "description": "–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É",
                "args": "<document_id> <–≤–æ–ø—Ä–æ—Å>",
                "handler": self.execute,
                "handler_kwargs": {"function_name": "ask_question"},
                "plugin_name": "text_document_qa"
            },
            {
                "command": "delete_document",
                "description": "–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç",
                "args": "<document_id>",
                "handler": self.execute,
                "handler_kwargs": {"function_name": "delete_document"},
                "plugin_name": "text_document_qa"
            }
        ]

    def get_spec(self) -> List[Dict]:
        return [{
            "name": "upload_document",
            "description": "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
            "parameters": {
                "type": "object",
                "properties": {
                    "file_content": {
                        "type": "string",
                        "description": "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"
                    },
                    "file_name": {
                        "type": "string",
                        "description": "–ò–º—è —Ñ–∞–π–ª–∞"
                    }
                },
                "required": ["file_content", "file_name"]
            }
        }, {
            "name": "list_documents",
            "description": "–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": []
            }
        }, {
            "name": "ask_question",
            "description": "–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É",
            "parameters": {
                "type": "object",
                "properties": {
                    "document_id": {
                        "type": "string",
                        "description": "ID –¥–æ–∫—É–º–µ–Ω—Ç–∞"
                    },
                    "query": {
                        "type": "string",
                        "description": "–í–æ–ø—Ä–æ—Å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É"
                    }
                },
                "required": ["document_id", "query"]
            }
        }, {
            "name": "delete_document",
            "description": "–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏ –µ–≥–æ –∏–Ω–¥–µ–∫—Å",
            "parameters": {
                "type": "object",
                "properties": {
                    "document_id": {
                        "type": "string",
                        "description": "ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è"
                    }
                },
                "required": ["document_id"]
            }
        }]

    def _load_existing_indices(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤"""
        for filename in os.listdir(self.index_dir):
            if filename.endswith('.index'):
                doc_id = filename[:-6]  # —É–¥–∞–ª—è–µ–º .index
                index_path = os.path.join(self.index_dir, filename)
                try:
                    index = faiss.read_index(index_path)
                    self.document_indices[doc_id] = index
                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞ {filename}: {e}")

    async def _create_document_index(self, text: str, doc_id: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
            docs = self.text_splitter.create_documents([text])
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            embeddings_response = await self.openai_helper.client.embeddings.create(
                model="text-embedding-ada-002",
                input=[doc.page_content for doc in docs],
                extra_headers={ "X-Title": "tgBot" },
            )
            
            embeddings = [item.embedding for item in embeddings_response.data]
            
            # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            dimension = len(embeddings[0])
            index = faiss.IndexFlatL2(dimension)
            index.add(np.array(embeddings))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
            index_path = os.path.join(self.index_dir, f"{doc_id}.index")
            faiss.write_index(index, index_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏
            chunks_path = os.path.join(self.docs_dir, f"{doc_id}.json")
            with open(chunks_path, 'w', encoding='utf-8') as f:
                json.dump([doc.page_content for doc in docs], f, ensure_ascii=False)
            
            self.document_indices[doc_id] = index
            return index
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {str(e)}")
            raise

    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
        if not self.cleanup_task:
            self.cleanup_task = asyncio.create_task(self._cleanup_loop())
            
    async def _cleanup_loop(self):
        """–ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        while True:
            try:
                await self._cleanup_old_documents()
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Å—Ç–∞—Ä—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏
            await asyncio.sleep(24 * 60 * 60)

    async def _update_last_access(self, doc_id: str):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É"""
        try:
            metadata_path = os.path.join(self.metadata_dir, f"{doc_id}.json")
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                metadata['last_accessed'] = time.time()
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥ –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –Ω–æ–≤–æ–º –æ–±—Ä–∞—â–µ–Ω–∏–∏
                metadata['warning_sent'] = False
                
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ—Å—Ç—É–ø–∞: {e}")

    async def _send_deletion_warning(self, metadata: Dict, doc_id: str):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –æ –ø—Ä–µ–¥—Å—Ç–æ—è—â–µ–º —É–¥–∞–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            chat_id = metadata.get('owner_chat_id')
            file_name = metadata.get('file_name')
            
            warning_message = {
                "direct_result": {
                    "kind": "text",
                    "format": "markdown",
                    "value": f"‚ö†Ô∏è *–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ–± —É–¥–∞–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞*\n\n"
                            f"–î–æ–∫—É–º–µ–Ω—Ç '*{file_name}*' –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª–µ–Ω —á–µ—Ä–µ–∑ 24 —á–∞—Å–∞.\n"
                            f"ID –¥–æ–∫—É–º–µ–Ω—Ç–∞: `{doc_id}`\n\n"
                            f"–ß—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç, –ø—Ä–æ—Å—Ç–æ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –Ω–µ–º—É —Å –ø–æ–º–æ—â—å—é –∫–æ–º–∞–Ω–¥—ã:\n"
                            f"`/ask_question {doc_id} –≤–∞—à_–≤–æ–ø—Ä–æ—Å`"
                }
            }
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π update —Å chat_id –¥–ª—è handle_direct_result
            update = type('Update', (), {'effective_chat': type('Chat', (), {'id': chat_id})})()
            await handle_direct_result(self.config, update, warning_message)
            
            # –û—Ç–º–µ—á–∞–µ–º –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –±—ã–ª–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ
            metadata['warning_sent'] = True
            metadata_path = os.path.join(self.metadata_dir, f"{doc_id}.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False)
                
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")

    async def _cleanup_old_documents(self):
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫ –∫–æ—Ç–æ—Ä—ã–º –Ω–µ –æ–±—Ä–∞—â–∞–ª–∏—Å—å –±–æ–ª—å—à–µ max_document_age"""
        current_time = time.time()
        deleted_count = 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        for filename in os.listdir(self.metadata_dir):
            if not filename.endswith('.json'):
                continue

            doc_id = filename[:-5]  # —É–¥–∞–ª—è–µ–º .json
            metadata_path = os.path.join(self.metadata_dir, filename)

            try:
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                last_accessed = metadata.get('last_accessed', metadata['created_at'])
                time_since_last_access = current_time - last_accessed
                
                # –ï—Å–ª–∏ –¥–æ —É–¥–∞–ª–µ–Ω–∏—è –æ—Å—Ç–∞–ª—Å—è –æ–¥–∏–Ω –¥–µ–Ω—å –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—â–µ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ
                if (time_since_last_access > (self.max_document_age - self.warning_before_delete) and 
                    time_since_last_access <= self.max_document_age and 
                    not metadata.get('warning_sent', False)):
                    await self._send_deletion_warning(metadata, doc_id)
                
                # –ï—Å–ª–∏ –ø—Ä–æ—à–ª–æ –±–æ–ª—å—à–µ max_document_age, —É–¥–∞–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
                elif time_since_last_access > self.max_document_age:
                    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
                    await self._delete_document_files(doc_id)
                    deleted_count += 1

            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {e}")

        if deleted_count > 0:
            logging.info(f"–£–¥–∞–ª–µ–Ω–æ {deleted_count} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    async def _delete_document_files(self, doc_id: str):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
        files_to_delete = [
            os.path.join(self.index_dir, f"{doc_id}.index"),
            os.path.join(self.docs_dir, f"{doc_id}.json"),
            os.path.join(self.metadata_dir, f"{doc_id}.json")
        ]

        for file_path in files_to_delete:
            if os.path.exists(file_path):
                try:
                    os.remove(file_path)
                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")

        # –£–¥–∞–ª—è–µ–º –∏–∑ –ø–∞–º—è—Ç–∏
        self.document_indices.pop(doc_id, None)

    async def _process_file(self, file_content: bytes, file_name: str) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        loop = asyncio.get_event_loop()
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(suffix=f".{file_name.split('.')[-1]}", delete=False) as temp_file:
            temp_file.write(file_content)
            temp_file.flush()
            temp_file_path = temp_file.name

        try:
            # –î–ª—è .txt —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–µ —á—Ç–µ–Ω–∏–µ
            if file_name.lower().endswith('.txt'):
                try:
                    with open(temp_file_path, 'r', encoding='utf-8') as f:
                        text_content = f.read()
                except UnicodeDecodeError:
                    with open(temp_file_path, 'r', encoding='windows-1251') as f:
                        text_content = f.read()
            else:
                # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                def convert_file():
                    result = self.document_converter.convert(temp_file_path)
                    return result.document.export_to_markdown()
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º CPU-bound –æ–ø–µ—Ä–∞—Ü–∏—é –≤ thread pool
                text_content = await loop.run_in_executor(self.executor, convert_file)

            return text_content

        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            try:
                os.remove(temp_file_path)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {e}")

    async def _check_document_access(self, doc_id: str, chat_id: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–º–µ–µ—Ç –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–æ—Å—Ç—É–ø –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É"""
        try:
            metadata_path = os.path.join(self.metadata_dir, f"{doc_id}.json")
            if not os.path.exists(metadata_path):
                return False
                
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            return metadata.get('owner_chat_id') == chat_id
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É: {e}")
            return False

    async def _get_user_documents(self, chat_id: str) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é"""
        documents = []
        try:
            logging.info(f"_get_user_documents –≤—ã–∑–≤–∞–Ω –¥–ª—è chat_id: {chat_id}")
            logging.info(f"–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {self.metadata_dir}")
            
            if not os.path.exists(self.metadata_dir):
                logging.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.metadata_dir}")
                return []
                
            files = os.listdir(self.metadata_dir)
            logging.info(f"–ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {files}")
            
            for filename in files:
                if not filename.endswith('.json'):
                    continue
                    
                metadata_path = os.path.join(self.metadata_dir, filename)
                logging.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {metadata_path}")
                
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    
                logging.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞ {filename}: {metadata}")
                logging.info(f"–°—Ä–∞–≤–Ω–∏–≤–∞–µ–º owner_chat_id: {metadata.get('owner_chat_id')} —Å chat_id: {chat_id}")
                
                if metadata.get('owner_chat_id') == chat_id:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è –≤ —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                    created_at = time.strftime('%Y-%m-%d %H:%M:%S', 
                                             time.localtime(metadata['created_at']))
                    documents.append({
                        'doc_id': metadata['doc_id'],
                        'file_name': metadata['file_name'],
                        'created_at': created_at,
                        'summary': metadata.get('summary', '')
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
            documents.sort(key=lambda x: x['created_at'], reverse=True)
            return documents
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []

    async def execute(self, function_name: str, helper, **kwargs) -> Dict:
        try:
            logging.info(f"TextDocumentQAPlugin.execute –≤—ã–∑–≤–∞–Ω —Å function_name={function_name}")
            logging.info(f"kwargs: {kwargs}")
            
            self.openai_helper = helper
            chat_id = kwargs.get('chat_id')
            logging.info(f"chat_id: {chat_id}")
            
            self.last_chat_id = chat_id  # –°–æ—Ö—Ä–∞–Ω—è–µ–º chat_id –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ execute
            await self.initialize()
            
            if function_name == "list_documents":
                logging.info("–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                documents = await self._get_user_documents(chat_id)
                logging.info(f"–ü–æ–ª—É—á–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã: {documents}")
                
                if not documents:
                    return {
                        "direct_result": {
                            "kind": "text",
                            "format": "markdown",
                            "value": "–£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
                        }
                    }
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                docs_list = ["–í–∞—à–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã:"]
                for doc in documents:
                    docs_list.append(f"\nüìÑ *{doc['file_name']}*")
                    docs_list.append(f"  ‚Ä¢ ID: `{doc['doc_id']}`")
                    docs_list.append(f"  ‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω: {doc['created_at']}")
                    if 'summary' in doc:
                        docs_list.append(f"  ‚Ä¢ –û–ø–∏—Å–∞–Ω–∏–µ: _{doc['summary']}_")
                    docs_list.append(f"  ‚Ä¢ –ö–æ–º–∞–Ω–¥—ã:")
                    docs_list.append(f"    `/ask_question {doc['doc_id']} –≤–∞—à_–≤–æ–ø—Ä–æ—Å` - –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å")
                    docs_list.append(f"    `/delete_document {doc['doc_id']}` - —É–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç")
                
                docs_list.append("\n–î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É `/list_documents`")
                
                return {
                    "direct_result": {
                        "kind": "text",
                        "format": "markdown",
                        "value": "\n".join(docs_list)
                    }
                }
                
            elif function_name == "upload_document":
                file_content = kwargs.get('file_content')
                file_name = kwargs.get('file_name')
                
                if not file_content:
                    return {"error": "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–æ"}

                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π ID –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                temp_id = hashlib.md5(str(time.time()).encode()).hexdigest()

                # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
                processing_task = asyncio.create_task(self._process_document(file_content, file_name, temp_id, kwargs.get('chat_id'), kwargs.get('update')))
                self.processing_tasks[temp_id] = processing_task

                return {
                    "direct_result": {
                        "kind": "text",
                        "format": "markdown",
                        "value": f"–ù–∞—á–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{file_name}'.\n–í—Ä–µ–º–µ–Ω–Ω—ã–π ID: `{temp_id}`\n\n–í—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –æ–±—â–µ–Ω–∏–µ —Å –±–æ—Ç–æ–º, –ø–æ–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è.\n–ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ä–∞–±–æ—Ç—ã."
                    }
                }

            elif function_name == "ask_question":
                doc_id = kwargs.get('document_id')
                query = kwargs.get('query')

                if doc_id not in self.document_indices:
                    return {"error": "–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"}

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞
                if not await self._check_document_access(doc_id, chat_id):
                    return {"error": "–£ –≤–∞—Å –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ —ç—Ç–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É"}

                # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                await self._update_last_access(doc_id)

                # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
                query_embedding_response = await helper.client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=query,
                    extra_headers={ "X-Title": "tgBot" },
                )
                query_embedding = query_embedding_response.data[0].embedding

                # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —á–∞–Ω–∫–∏
                index = self.document_indices[doc_id]
                k = 3  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∏–∂–∞–π—à–∏—Ö —á–∞–Ω–∫–æ–≤
                D, I = index.search(np.array([query_embedding]), k)

                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏
                chunks_path = os.path.join(self.docs_dir, f"{doc_id}.json")
                with open(chunks_path, 'r', encoding='utf-8') as f:
                    chunks = json.load(f)
                logging.info(f"–ù–∞–π–¥–µ–Ω—ã —á–∞–Ω–∫–∏: {chunks}")

                # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
                context = "\n".join([chunks[i] for i in I[0]])

                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è GPT
                prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å.
                –ö–æ–Ω—Ç–µ–∫—Å—Ç:
                {context}

                –í–æ–ø—Ä–æ—Å: {query}
                """

                # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç GPT
                response, _ = await helper.get_chat_response(
                    chat_id=chat_id,
                    query=prompt
                )

                return {
                    "direct_result": {
                        "kind": "text",
                        "format": "markdown",
                        "value": response
                    }
                }

            elif function_name == "delete_document":
                doc_id = kwargs.get('document_id')
                
                if doc_id not in self.document_indices:
                    return {"error": "–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"}

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞
                if not await self._check_document_access(doc_id, chat_id):
                    return {"error": "–£ –≤–∞—Å –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ —ç—Ç–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É"}

                # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
                await self._delete_document_files(doc_id)

                return {
                    "direct_result": {
                        "kind": "text",
                        "format": "markdown",
                        "value": "–î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω"
                    }
                }

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –≤ TextDocumentQAPlugin: {str(e)}")
            return {"error": f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"} 

    async def _process_document(self, file_content: bytes, file_name: str, temp_id: str, chat_id: str, update=None):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
            text_content = await self._process_file(file_content, file_name)

            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_id = hashlib.md5(chat_id.encode() + text_content.encode()).hexdigest()

            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
            await self._create_document_index(text_content, doc_id)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∞–º–º–∞—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            summary_prompt = f"–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (–Ω–µ –±–æ–ª–µ–µ 150-200 —Å–∏–º–≤–æ–ª–æ–≤) –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n\n{text_content[:5000]}"
            summary_response, _ = await self.openai_helper.ask(
                prompt=summary_prompt,
                user_id=chat_id,
                assistant_prompt="–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –¢—ã —É–º–µ–µ—à—å —Å–æ–∑–¥–∞–≤–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ, –Ω–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø–∏—Å—ã–≤–∞—é—â–µ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏–µ."
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            current_time = time.time()
            metadata = {
                'file_name': file_name,
                'created_at': current_time,
                'last_accessed': current_time,
                'doc_id': doc_id,
                'owner_chat_id': chat_id,
                'summary': summary_response
            }
            metadata_path = os.path.join(self.metadata_dir, f"{doc_id}.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False)

            gc.collect()
            torch.cuda.empty_cache()

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ chat_response
            response = {
                "direct_result": {
                    "kind": "text",
                    "format": "markdown",
                    "value": f"–î–æ–∫—É–º–µ–Ω—Ç '*{file_name}*' —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω.\n"
                            f"üìù –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: _{summary_response}_\n\n"
                            f"üìù –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
                            f"‚Ä¢ `/ask_question {doc_id} –≤–∞—à_–≤–æ–ø—Ä–æ—Å` - –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É\n"
                            f"‚Ä¢ `/delete_document {doc_id}` - —É–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç\n"
                            f"‚Ä¢ `/list_documents` - –ø–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n\n"
                            f"–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –¥–æ–∫—É–º–µ–Ω—Ç –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª–µ–Ω —á–µ—Ä–µ–∑ 30 –¥–Ω–µ–π –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ –Ω–µ–º—É."
                }
            }
            await handle_direct_result(self.config, update, response)

        except Exception as e:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
            error_response = {
                "direct_result": {
                    "kind": "text",
                    "format": "markdown",
                    "value": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{file_name}': {str(e)}"
                }
            }
            await handle_direct_result(self.config, update, error_response)
        finally:
            # –£–¥–∞–ª—è–µ–º –∑–∞–¥–∞—á—É –∏–∑ —Å–ª–æ–≤–∞—Ä—è
            self.processing_tasks.pop(temp_id, None) 